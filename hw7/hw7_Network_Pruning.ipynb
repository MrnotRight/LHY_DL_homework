{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"hw7_Network_Pruning.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"b5cFq_TgWlQ_","colab_type":"text"},"source":["# Homework 7 - Network Compression (Network Pruning)\n","\n","> Author: Arvin Liu (b05902127@ntu.edu.tw)\n","\n","若有任何問題，歡迎來信至助教信箱 ntu-ml-2020spring-ta@googlegroups.com"]},{"cell_type":"markdown","metadata":{"id":"vNiZCGrIYKdR","colab_type":"text"},"source":["# Readme\n","\n","\n","HW7的任務是模型壓縮 - Neural Network Compression。\n","\n","Compression有很多種門派，在這裡我們會介紹上課出現過的其中四種，分別是:\n","\n","* 知識蒸餾 Knowledge Distillation\n","* 網路剪枝 Network Pruning\n","* 用少量參數來做CNN Architecture Design\n","* 參數量化 Weight Quantization\n","\n","在這個notebook中我們會介紹Network Pruning，\n","而我們有提供已經做完Knowledge Distillation的小model來做Pruning。\n","\n","* Model架構 / Architecute Design在同目錄中的hw7_Architecture_Design.ipynb。\n","* 下載已經train好的小model(0.99M): https://drive.google.com/open?id=12wtIa0WVRcpboQzhgRUJOpcXe23tgWUL\n","  * 參數為 base=16, width_mult=1 (default)"]},{"cell_type":"code","metadata":{"id":"XdzskhdEb65Z","colab_type":"code","outputId":"547253ea-4835-4e77-a8eb-0e48ffb86141","executionInfo":{"status":"ok","timestamp":1583664586092,"user_tz":-480,"elapsed":78968,"user":{"displayName":"李宏毅","photoUrl":"","userId":"14645962671355321712"}},"colab":{"base_uri":"https://localhost:8080/","height":84}},"source":["import torch\n","import os\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","import torchvision.models as models"],"execution_count":14,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bdUtCxBBcH0B","colab_type":"text"},"source":["Network Pruning\n","===\n","在這裡我們會教Neuron Pruning。\n","<img src=\"https://i.imgur.com/Iwp90Wp.png\" width=\"500px\">\n","\n","簡單上來說就是讓一個已經學完的model中的neuron進行刪減，讓整個網路變得更瘦。\n","\n","## Weight & Neuron Pruning\n","* weight和neuron pruning差別在於prune掉一個neuron就等於是把一個matrix的整個column全部砍掉。但如此一來速度就會比較快。因為neuron pruning後matrix整體變小，但weight pruning大小不變，只是有很多空洞。\n","\n","## What to Prune?\n","* 既然要Neuron Pruning，那就必須要先衡量Neuron的重要性。衡量完所有的Neuron後，就可以把比較不重要的Neuron刪減掉。\n","* 在這裡我們介紹一個很簡單可以衡量Neuron重要性的方法 - 就是看batchnorm layer的$\\gamma$因子來決定neuron的重要性。 (by paper - Network Slimming)\n","  ![](https://i.imgur.com/JVpCm2r.png)\n","* 相信大家看這個pytorch提供的batchnorm公式應該就可以意識到為甚麼$\\gamma$可以當作重要性來衡量了:)\n","\n","* Network Slimming其實步驟沒有這麼簡單，有興趣的同學可以check以下連結。[Netowrk Slimming](https://arxiv.org/abs/1708.06519)\n","\n","\n","## 為甚麼這會 work?\n","* 樹多必有枯枝，有些neuron只是在躺分，所以有他沒他沒有差。\n","* 困難的說可以回想起老師說過的大樂透假說(The Lottery Ticket Hypothesis)就可以知道了。\n","\n","## 要怎麼實作?\n","* 為了避免複雜的操作，我們會將StudentNet(width_mult=$\\alpha$)的neuron經過篩選後移植到StudentNet(width_mult=$\\beta$)。($\\alpha > \\beta$)\n","* 篩選的方法也很簡單，只需要抓出每一個block的batchnorm的$\\gamma$即可。\n","\n","## 一些實作細節\n","* 假設model中間兩層是這樣的:\n","\n","|Layer|Output # of Channels|\n","|-|-|\n","|Input|in_chs|\n","|Depthwise(in_chs)|in_chs|\n","|BatchNorm(in_chs)|in_chs|\n","|Pointwise(in_chs, **mid_chs**)|**mid_chs**|\n","|**Depthwise(mid_chs)**|**mid_chs**|\n","|**BatchNorm(mid_chs)**|**mid_chs**|\n","|Pointwise(**mid_chs**, out_chs)|out_chs|\n","\n","則你會發現利用第二個BatchNorm來做篩選的時候，跟他的Neuron有直接關係的是該層的Depthwise&Pointwise以及上層的Pointwise。\n","因此再做neuron篩選時記得要將這四個(包括自己, bn)也要同時prune掉。\n","\n","* 在Design Architecure內，model的一個block，名稱所對應的Weight；\n","\n","|#|name|meaning|code|weight shape|\n","|-|-|-|-|-|\n","|0|cnn.{i}.0|Depthwise Convolution Layer|nn.Conv2d(x, x, 3, 1, 1, group=x)|(x, 1, 3, 3)|\n","|1|cnn.{i}.1|Batch Normalization|nn.BatchNorm2d(x)|(x)|\n","|2||ReLU6|nn.ReLU6||\n","|3|cnn.{i}.3|Pointwise Convolution Layer|nn.Conv2d(x, y, 1),|(y, x, 1, 1)|\n","|4||MaxPooling|nn.MaxPool2d(2, 2, 0)||"]},{"cell_type":"code","metadata":{"id":"M-dSi_P-4les","colab_type":"code","colab":{}},"source":["def network_slimming(old_model, new_model):\n","    params = old_model.state_dict()\n","    new_params = new_model.state_dict()\n","    \n","    # selected_idx: 每一層所選擇的neuron index\n","    selected_idx = []\n","    # 我們總共有7層CNN，因此逐一抓取選擇的neuron index們。\n","    for i in range(8):\n","        # 根據上表，我們要抓的gamma係數在cnn.{i}.1.weight內。\n","        importance = params[f'cnn.{i}.1.weight']\n","        # 抓取總共要篩選幾個neuron。\n","        old_dim = len(importance)\n","        new_dim = len(new_params[f'cnn.{i}.1.weight'])\n","        # 以Ranking做Index排序，較大的會在前面(descending=True)。\n","        ranking = torch.argsort(importance, descending=True)\n","        # 把篩選結果放入selected_idx中。\n","        selected_idx.append(ranking[:new_dim])\n","\n","    now_processed = 1\n","    for (name, p1), (name2, p2) in zip(params.items(), new_params.items()):\n","        # 如果是cnn層，則移植參數。\n","        # 如果是FC層，或是該參數只有一個數字(例如batchnorm的tracenum等等資訊)，那麼就直接複製。\n","        if name.startswith('cnn') and p1.size() != torch.Size([]) and now_processed != len(selected_idx):\n","            # 當處理到Pointwise的weight時，讓now_processed+1，表示該層的移植已經完成。\n","            if name.startswith(f'cnn.{now_processed}.3'):\n","                now_processed += 1\n","\n","            # 如果是pointwise，weight會被上一層的pruning和下一層的pruning所影響，因此需要特判。\n","            if name.endswith('3.weight'):\n","                # 如果是最後一層cnn，則輸出的neuron不需要prune掉。\n","                if len(selected_idx) == now_processed:\n","                    new_params[name] = p1[:,selected_idx[now_processed-1]]\n","                # 反之，就依照上層和下層所選擇的index進行移植。\n","                # 這裡需要注意的是Conv2d(x,y,1)的weight shape是(y,x,1,1)，順序是反的。\n","                else:\n","                    new_params[name] = p1[selected_idx[now_processed]][:,selected_idx[now_processed-1]]\n","            else:\n","                new_params[name] = p1[selected_idx[now_processed]]\n","        else:\n","            new_params[name] = p1\n","\n","    # 讓新model load進被我們篩選過的parameters，並回傳new_model。        \n","    new_model.load_state_dict(new_params)\n","    return new_model"],"execution_count":15,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NfnRoOt5VIze","colab_type":"text"},"source":["# Data Processing\n","\n","我們的Dataset使用的是跟Hw3 - CNN同樣的Dataset，因此這個區塊的Augmentation / Read Image大家參考就好。\n","\n","如果有不會的話可以回去看Hw3的colab。"]},{"cell_type":"code","metadata":{"id":"ExdUvTRaVNOT","colab_type":"code","colab":{}},"source":["import re\n","import torch\n","from glob import glob\n","from PIL import Image\n","import torchvision.transforms as transforms\n","\n","class MyDataset(torch.utils.data.Dataset):\n","    def __init__(self, folderName, transform=None):\n","        self.transform = transform\n","        self.data = []\n","        self.label = []\n","\n","        for img_path in sorted(glob(folderName + '/*.jpg')):\n","            try:\n","                # Get classIdx by parsing image path\n","                class_idx = int(re.findall(re.compile(r'\\d+'), img_path)[1])\n","            except:\n","                # if inference mode (there's no answer), class_idx default 0\n","                class_idx = 0\n"," \n","            image = Image.open(img_path)\n","            # Get File Descriptor\n","            image_fp = image.fp\n","            image.load()\n","            # Close File Descriptor (or it'll reach OPEN_MAX)\n","            image_fp.close()\n","\n","            self.data.append(image)\n","            self.label.append(class_idx)\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        if torch.is_tensor(idx):\n","            idx = idx.tolist()\n","        image = self.data[idx]\n","        if self.transform:\n","            image = self.transform(image)\n","        return image, self.label[idx]\n","\n","\n","trainTransform = transforms.Compose([\n","    transforms.RandomCrop(256, pad_if_needed=True, padding_mode='symmetric'),\n","    transforms.RandomHorizontalFlip(),\n","    transforms.RandomRotation(15),\n","    transforms.ToTensor(),\n","])\n","testTransform = transforms.Compose([\n","    transforms.CenterCrop(256),\n","    transforms.ToTensor(),\n","])\n","\n","def get_dataloader(mode='training', batch_size=32):\n","    assert mode in ['training', 'testing', 'validation']\n","\n","    dataset = MyDataset(\n","        f'../food-11/{mode}',\n","        transform=trainTransform if mode == 'training' else testTransform)\n","\n","    dataloader = torch.utils.data.DataLoader(\n","        dataset,\n","        batch_size=batch_size,\n","        shuffle=(mode == 'training'))\n","\n","    return dataloader"],"execution_count":16,"outputs":[]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[],"source":["import torch.nn as nn\n","import torch.nn.functional as F\n","import torch\n","\n","class StudentNet(nn.Module):\n","    '''\n","      在這個Net裡面，我們會使用Depthwise & Pointwise Convolution Layer來疊model。\n","      你會發現，將原本的Convolution Layer換成Dw & Pw後，Accuracy通常不會降很多。\n","      另外，取名為StudentNet是因為這個Model等會要做Knowledge Distillation。\n","    '''\n","    def __init__(self, base=16, width_mult=1):\n","        '''\n","          Args:\n","            base: 這個model一開始的ch數量，每過一層都會*2，直到base*16為止。\n","            width_mult: 為了之後的Network Pruning使用，在base*8 chs的Layer上會 * width_mult代表剪枝後的ch數量。        \n","        '''\n","        super(StudentNet, self).__init__()\n","        multiplier = [1, 2, 4, 8, 16, 16, 16, 16]\n","\n","        # bandwidth: 每一層Layer所使用的ch數量\n","        bandwidth = [ base * m for m in multiplier]\n","\n","        # 我們只Pruning第三層以後的Layer\n","        for i in range(3, 7):\n","            bandwidth[i] = int(bandwidth[i] * width_mult)\n","\n","        self.cnn = nn.Sequential(\n","            # 第一層我們通常不會拆解Convolution Layer。\n","            nn.Sequential(\n","                nn.Conv2d(3, bandwidth[0], 3, 1, 1),\n","                nn.BatchNorm2d(bandwidth[0]),\n","                nn.ReLU6(),\n","                nn.MaxPool2d(2, 2, 0),\n","            ),\n","            # 接下來每一個Sequential Block都一樣，所以我們只講一個Block\n","            nn.Sequential(\n","                # Depthwise Convolution\n","                nn.Conv2d(bandwidth[0], bandwidth[0], 3, 1, 1, groups=bandwidth[0]),\n","                # Batch Normalization\n","                nn.BatchNorm2d(bandwidth[0]),\n","                # ReLU6 是限制Neuron最小只會到0，最大只會到6。 MobileNet系列都是使用ReLU6。\n","                # 使用ReLU6的原因是因為如果數字太大，會不好壓到float16 / or further qunatization，因此才給個限制。\n","                nn.ReLU6(),\n","                # Pointwise Convolution\n","                nn.Conv2d(bandwidth[0], bandwidth[1], 1),\n","                # 過完Pointwise Convolution不需要再做ReLU，經驗上Pointwise + ReLU效果都會變差。\n","                nn.MaxPool2d(2, 2, 0),\n","                # 每過完一個Block就Down Sampling\n","            ),\n","\n","            nn.Sequential(\n","                nn.Conv2d(bandwidth[1], bandwidth[1], 3, 1, 1, groups=bandwidth[1]),\n","                nn.BatchNorm2d(bandwidth[1]),\n","                nn.ReLU6(),\n","                nn.Conv2d(bandwidth[1], bandwidth[2], 1),\n","                nn.MaxPool2d(2, 2, 0),\n","            ),\n","\n","            nn.Sequential(\n","                nn.Conv2d(bandwidth[2], bandwidth[2], 3, 1, 1, groups=bandwidth[2]),\n","                nn.BatchNorm2d(bandwidth[2]),\n","                nn.ReLU6(),\n","                nn.Conv2d(bandwidth[2], bandwidth[3], 1),\n","                nn.MaxPool2d(2, 2, 0),\n","            ),\n","\n","            # 到這邊為止因為圖片已經被Down Sample很多次了，所以就不做MaxPool\n","            nn.Sequential(\n","                nn.Conv2d(bandwidth[3], bandwidth[3], 3, 1, 1, groups=bandwidth[3]),\n","                nn.BatchNorm2d(bandwidth[3]),\n","                nn.ReLU6(),\n","                nn.Conv2d(bandwidth[3], bandwidth[4], 1),\n","            ),\n","\n","            nn.Sequential(\n","                nn.Conv2d(bandwidth[4], bandwidth[4], 3, 1, 1, groups=bandwidth[4]),\n","                nn.BatchNorm2d(bandwidth[4]),\n","                nn.ReLU6(),\n","                nn.Conv2d(bandwidth[4], bandwidth[5], 1),\n","            ),\n","\n","            nn.Sequential(\n","                nn.Conv2d(bandwidth[5], bandwidth[5], 3, 1, 1, groups=bandwidth[5]),\n","                nn.BatchNorm2d(bandwidth[5]),\n","                nn.ReLU6(),\n","                nn.Conv2d(bandwidth[5], bandwidth[6], 1),\n","            ),\n","\n","            nn.Sequential(\n","                nn.Conv2d(bandwidth[6], bandwidth[6], 3, 1, 1, groups=bandwidth[6]),\n","                nn.BatchNorm2d(bandwidth[6]),\n","                nn.ReLU6(),\n","                nn.Conv2d(bandwidth[6], bandwidth[7], 1),\n","            ),\n","\n","            # 這邊我們採用Global Average Pooling。\n","            # 如果輸入圖片大小不一樣的話，就會因為Global Average Pooling壓成一樣的形狀，這樣子接下來做FC就不會對不起來。\n","            nn.AdaptiveAvgPool2d((1, 1)),\n","        )\n","        self.fc = nn.Sequential(\n","            # 這邊我們直接Project到11維輸出答案。\n","            nn.Linear(bandwidth[7], 11),\n","        )\n","\n","    def forward(self, x):\n","        out = self.cnn(x)\n","        out = out.view(out.size()[0], -1)\n","        return self.fc(out)"]},{"cell_type":"markdown","metadata":{"id":"ACPwL9_JWceQ","colab_type":"text"},"source":["# Pre-processing\n","\n","我們已經提供原始小model binary，架構是hw7_Architecture_Design.ipynb中的StudentNet。"]},{"cell_type":"code","metadata":{"id":"wzuuGvnbWkG8","colab_type":"code","colab":{}},"source":["# get dataloader\n","train_dataloader = get_dataloader('training', batch_size=32)\n","valid_dataloader = get_dataloader('validation', batch_size=32)"],"execution_count":17,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZWdQtDtgoGCp","colab_type":"code","outputId":"009c4b7a-0eb1-4636-8ccc-96a93d32be26","executionInfo":{"status":"ok","timestamp":1583664665857,"user_tz":-480,"elapsed":158720,"user":{"displayName":"李宏毅","photoUrl":"","userId":"14645962671355321712"}},"colab":{"base_uri":"https://localhost:8080/","height":84}},"source":["net = StudentNet().cuda()\n","net.load_state_dict(torch.load('student_custom_small.bin'))\n","\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.AdamW(net.parameters(), lr=1e-3)"],"execution_count":21,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Wvc1W5yO2QaE","colab_type":"text"},"source":["# Start Training\n","\n","* 每次Prune rate是0.95，Prune完後會重新fine-tune 3 epochs。\n","* 其餘的步驟與你在做Hw3 - CNN的時候一樣。\n","\n"]},{"cell_type":"code","metadata":{"id":"-TzmWtT62Qmy","colab_type":"code","outputId":"f1687892-bc47-4605-fd4a-34d9701a8aa0","executionInfo":{"status":"ok","timestamp":1583666000953,"user_tz":-480,"elapsed":1060656,"user":{"displayName":"李宏毅","photoUrl":"","userId":"14645962671355321712"}},"colab":{"base_uri":"https://localhost:8080/","height":437},"tags":[]},"source":["def run_epoch(dataloader, update=True, alpha=0.5):\n","    total_num, total_hit, total_loss = 0, 0, 0\n","    for now_step, batch_data in enumerate(dataloader):\n","        # 清空 optimizer\n","        optimizer.zero_grad()\n","        # 處理 input\n","        inputs, labels = batch_data\n","        inputs = inputs.cuda()\n","        labels = labels.cuda()\n","  \n","        logits = net(inputs)\n","        loss = criterion(logits, labels)\n","        if update:\n","            loss.backward()\n","            optimizer.step()\n","\n","        total_hit += torch.sum(torch.argmax(logits, dim=1) == labels).item()\n","        total_num += len(inputs)\n","        total_loss += loss.item() * len(inputs)\n","\n","    return total_loss / total_num, total_hit / total_num\n","\n","now_width_mult = 1\n","for i in range(5):\n","    now_width_mult *= 0.95\n","    new_net = StudentNet(width_mult=now_width_mult).cuda()\n","    params = net.state_dict()\n","    net = network_slimming(net, new_net)\n","    now_best_acc = 0\n","    for epoch in range(5):\n","        net.train()\n","        train_loss, train_acc = run_epoch(train_dataloader, update=True)\n","        net.eval()\n","        valid_loss, valid_acc = run_epoch(valid_dataloader, update=False)\n","        # 在每個width_mult的情況下，存下最好的model。\n","        if valid_acc > now_best_acc:\n","            now_best_acc = valid_acc\n","            torch.save(net.state_dict(), f'custom_small_rate_{now_width_mult}.bin')\n","        print('rate {:6.4f} epoch {:>3d}: train loss: {:6.4f}, acc {:6.4f} valid loss: {:6.4f}, acc {:6.4f}'.format(now_width_mult, epoch, train_loss, train_acc, valid_loss, valid_acc))"],"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":"rate 0.9500 epoch   0: train loss: 0.4973, acc 0.8666 valid loss: 1.1072, acc 0.7994\nrate 0.9500 epoch   1: train loss: 0.4796, acc 0.8684 valid loss: 1.1279, acc 0.7971\nrate 0.9500 epoch   2: train loss: 0.4896, acc 0.8650 valid loss: 1.1155, acc 0.8000\nrate 0.9500 epoch   3: train loss: 0.4818, acc 0.8653 valid loss: 1.1269, acc 0.7968\nrate 0.9500 epoch   4: train loss: 0.4708, acc 0.8666 valid loss: 1.1183, acc 0.7974\nrate 0.9025 epoch   0: train loss: 0.6030, acc 0.8406 valid loss: 1.1883, acc 0.7854\nrate 0.9025 epoch   1: train loss: 0.5978, acc 0.8402 valid loss: 1.1776, acc 0.7837\nrate 0.9025 epoch   2: train loss: 0.5732, acc 0.8460 valid loss: 1.1977, acc 0.7799\nrate 0.9025 epoch   3: train loss: 0.5899, acc 0.8377 valid loss: 1.1804, acc 0.7784\nrate 0.9025 epoch   4: train loss: 0.5691, acc 0.8426 valid loss: 1.1727, acc 0.7840\nrate 0.8574 epoch   0: train loss: 0.7008, acc 0.8102 valid loss: 1.2388, acc 0.7630\nrate 0.8574 epoch   1: train loss: 0.7015, acc 0.8116 valid loss: 1.1983, acc 0.7694\nrate 0.8574 epoch   2: train loss: 0.6893, acc 0.8126 valid loss: 1.2494, acc 0.7630\nrate 0.8574 epoch   3: train loss: 0.6693, acc 0.8199 valid loss: 1.2167, acc 0.7630\nrate 0.8574 epoch   4: train loss: 0.6771, acc 0.8107 valid loss: 1.1715, acc 0.7665\nrate 0.8145 epoch   0: train loss: 0.8400, acc 0.7746 valid loss: 1.2449, acc 0.7411\nrate 0.8145 epoch   1: train loss: 0.8100, acc 0.7753 valid loss: 1.3192, acc 0.7341\nrate 0.8145 epoch   2: train loss: 0.8394, acc 0.7729 valid loss: 1.3374, acc 0.7309\nrate 0.8145 epoch   3: train loss: 0.8260, acc 0.7775 valid loss: 1.3188, acc 0.7394\nrate 0.8145 epoch   4: train loss: 0.8429, acc 0.7695 valid loss: 1.2789, acc 0.7394\nrate 0.7738 epoch   0: train loss: 1.0133, acc 0.7319 valid loss: 1.4575, acc 0.7009\nrate 0.7738 epoch   1: train loss: 1.0148, acc 0.7323 valid loss: 1.4192, acc 0.7067\nrate 0.7738 epoch   2: train loss: 1.0242, acc 0.7305 valid loss: 1.4865, acc 0.6991\nrate 0.7738 epoch   3: train loss: 1.0185, acc 0.7293 valid loss: 1.3704, acc 0.7117\nrate 0.7738 epoch   4: train loss: 1.0155, acc 0.7316 valid loss: 1.4440, acc 0.7055\n"}]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}]}