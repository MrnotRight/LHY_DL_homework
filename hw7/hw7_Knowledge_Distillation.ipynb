{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"hw7_Knowledge_Distillation.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"b5cFq_TgWlQ_","colab_type":"text"},"source":["# Homework 7 - Network Compression (Knowledge Distillation)\n","\n","> Author: Arvin Liu (b05902127@ntu.edu.tw)\n","\n","若有任何問題，歡迎來信至助教信箱 ntu-ml-2020spring-ta@googlegroups.com"]},{"cell_type":"markdown","metadata":{"id":"vNiZCGrIYKdR","colab_type":"text"},"source":["# Readme\n","\n","\n","HW7的任務是模型壓縮 - Neural Network Compression。\n","\n","Compression有很多種門派，在這裡我們會介紹上課出現過的其中四種，分別是:\n","\n","* 知識蒸餾 Knowledge Distillation\n","* 網路剪枝 Network Pruning\n","* 用少量參數來做CNN Architecture Design\n","* 參數量化 Weight Quantization\n","\n","在這個notebook中我們會介紹Knowledge Distillation，\n","而我們有提供已經學習好的大model方便大家做Knowledge Distillation。\n","而我們使用的小model是\"Architecture Design\"過的model。\n","\n","* Architecute Design在同目錄中的hw7_Architecture_Design.ipynb。\n","* 下載pretrained大model(47.2M): https://drive.google.com/file/d/1B8ljdrxYXJsZv2vmTequdPOofp3VF3NN/view?usp=sharing\n","  * 請使用torchvision提供的ResNet18，把num_classes改成11後load進去即可。(後面有範例。)"]},{"cell_type":"code","metadata":{"id":"XdzskhdEb65Z","colab_type":"code","outputId":"d856fd31-a1e3-4653-b368-5958c55a869f","executionInfo":{"status":"ok","timestamp":1583664561144,"user_tz":-480,"elapsed":77143,"user":{"displayName":"李宏毅","photoUrl":"","userId":"14645962671355321712"}},"colab":{"base_uri":"https://localhost:8080/","height":84}},"source":["import torch\n","import os\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","import torchvision.models as models"],"execution_count":1,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bdUtCxBBcH0B","colab_type":"text"},"source":["Knowledge Distillation\n","===\n","\n","<img src=\"https://i.imgur.com/H2aF7Rv.png=100x\" width=\"500px\">\n","\n","簡單上來說就是讓已經做得很好的大model們去告訴小model\"如何\"學習。\n","而我們如何做到這件事情呢? 就是利用大model預測的logits給小model當作標準就可以了。\n","\n","## 為甚麼這會work?\n","* 例如當data不是很乾淨的時候，對一般的model來說他是個noise，只會干擾學習。透過去學習其他大model預測的logits會比較好。\n","* label和label之間可能有關連，這可以引導小model去學習。例如數字8可能就和6,9,0有關係。\n","* 弱化已經學習不錯的target(?)，避免讓其gradient干擾其他還沒學好的task。\n","\n","\n","## 要怎麼實作?\n","* $Loss = \\alpha T^2 \\times KL(\\frac{\\text{Teacher's Logits}}{T} || \\frac{\\text{Student's Logits}}{T}) + (1-\\alpha)(\\text{原本的Loss})$\n","\n","\n","* 以下code為甚麼要對student使用log_softmax: https://github.com/peterliht/knowledge-distillation-pytorch/issues/2\n","* reference: [Distilling the Knowledge in a Neural Network](https://arxiv.org/abs/1503.02531)"]},{"cell_type":"code","metadata":{"id":"M-dSi_P-4les","colab_type":"code","colab":{}},"source":["def loss_fn_kd(outputs, labels, teacher_outputs, T=20, alpha=0.5):\n","    # 一般的Cross Entropy\n","    hard_loss = F.cross_entropy(outputs, labels) * (1. - alpha)\n","    # 讓logits的log_softmax對目標機率(teacher的logits/T後softmax)做KL Divergence。\n","    soft_loss = nn.KLDivLoss(reduction='batchmean')(F.log_softmax(outputs/T, dim=1),\n","                             F.softmax(teacher_outputs/T, dim=1)) * (alpha * T * T)\n","    return hard_loss + soft_loss"],"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NfnRoOt5VIze","colab_type":"text"},"source":["# Data Processing\n","\n","我們的Dataset使用的是跟Hw3 - CNN同樣的Dataset，因此這個區塊的Augmentation / Read Image大家參考或直接抄就好。\n","\n","如果有不會的話可以回去看Hw3的colab。\n","\n","需要注意的是如果要自己寫的話，Augment的方法最好使用我們的方法，避免輸入有差異導致Teacher Net預測不好。"]},{"cell_type":"code","metadata":{"id":"ExdUvTRaVNOT","colab_type":"code","colab":{}},"source":["import re\n","import torch\n","from glob import glob\n","from PIL import Image\n","import torchvision.transforms as transforms\n","\n","class MyDataset(torch.utils.data.Dataset):\n","    def __init__(self, folderName, transform=None):\n","        self.transform = transform\n","        self.data = []\n","        self.label = []\n","\n","        for img_path in sorted(glob(folderName + '/*.jpg')):\n","            try:\n","                # Get classIdx by parsing image path\n","                class_idx = int(re.findall(re.compile(r'\\d+'), img_path)[1])\n","            except:\n","                # if inference mode (there's no answer), class_idx default 0\n","                class_idx = 0\n","\n","            image = Image.open(img_path)\n","            # Get File Descriptor\n","            image_fp = image.fp\n","            image.load()\n","            # Close File Descriptor (or it'll reach OPEN_MAX)\n","            image_fp.close()\n","\n","            self.data.append(image)\n","            self.label.append(class_idx)\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        if torch.is_tensor(idx):\n","            idx = idx.tolist()\n","        image = self.data[idx]\n","        if self.transform:\n","            image = self.transform(image)\n","        return image, self.label[idx]\n","\n","\n","trainTransform = transforms.Compose([\n","    transforms.RandomCrop(256, pad_if_needed=True, padding_mode='symmetric'),\n","    transforms.RandomHorizontalFlip(),\n","    transforms.RandomRotation(15),\n","    transforms.ToTensor(),\n","])\n","testTransform = transforms.Compose([\n","    transforms.CenterCrop(256),\n","    transforms.ToTensor(),\n","])\n","\n","def get_dataloader(mode='training', batch_size=32):\n","    assert mode in ['training', 'testing', 'validation']\n","    dataset = MyDataset(\n","        f'../food-11/{mode}',\n","        transform=trainTransform if mode == 'training' else testTransform)\n","    dataloader = torch.utils.data.DataLoader(\n","        dataset,\n","        batch_size=batch_size,\n","        shuffle=(mode == 'training'))\n","    return dataloader"],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ACPwL9_JWceQ","colab_type":"text"},"source":["# Pre-processing\n","\n","我們已經提供TeacherNet的state_dict，其架構是torchvision提供的ResNet18。\n","\n","至於StudentNet的架構則在hw7_Architecture_Design.ipynb中。\n","\n","這裡我們使用的Optimizer為AdamW，沒有為甚麼，就純粹我想用。"]},{"cell_type":"code","metadata":{"id":"wzuuGvnbWkG8","colab_type":"code","colab":{}},"source":["# get dataloader\n","train_dataloader = get_dataloader('training', batch_size=32)\n","valid_dataloader = get_dataloader('validation', batch_size=32)"],"execution_count":6,"outputs":[]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["import torch.nn as nn\n","import torch.nn.functional as F\n","import torch\n","\n","class StudentNet(nn.Module):\n","    '''\n","      在這個Net裡面，我們會使用Depthwise & Pointwise Convolution Layer來疊model。\n","      你會發現，將原本的Convolution Layer換成Dw & Pw後，Accuracy通常不會降很多。\n","      另外，取名為StudentNet是因為這個Model等會要做Knowledge Distillation。\n","    '''\n","    def __init__(self, base=16, width_mult=1):\n","        '''\n","          Args:\n","            base: 這個model一開始的ch數量，每過一層都會*2，直到base*16為止。\n","            width_mult: 為了之後的Network Pruning使用，在base*8 chs的Layer上會 * width_mult代表剪枝後的ch數量。        \n","        '''\n","        super(StudentNet, self).__init__()\n","        multiplier = [1, 2, 4, 8, 16, 16, 16, 16]\n","\n","        # bandwidth: 每一層Layer所使用的ch數量\n","        bandwidth = [ base * m for m in multiplier]\n","\n","        # 我們只Pruning第三層以後的Layer\n","        for i in range(3, 7):\n","            bandwidth[i] = int(bandwidth[i] * width_mult)\n","\n","        self.cnn = nn.Sequential(\n","            # 第一層我們通常不會拆解Convolution Layer。\n","            nn.Sequential(\n","                nn.Conv2d(3, bandwidth[0], 3, 1, 1),\n","                nn.BatchNorm2d(bandwidth[0]),\n","                nn.ReLU6(),\n","                nn.MaxPool2d(2, 2, 0),\n","            ),\n","            # 接下來每一個Sequential Block都一樣，所以我們只講一個Block\n","            nn.Sequential(\n","                # Depthwise Convolution\n","                nn.Conv2d(bandwidth[0], bandwidth[0], 3, 1, 1, groups=bandwidth[0]),\n","                # Batch Normalization\n","                nn.BatchNorm2d(bandwidth[0]),\n","                # ReLU6 是限制Neuron最小只會到0，最大只會到6。 MobileNet系列都是使用ReLU6。\n","                # 使用ReLU6的原因是因為如果數字太大，會不好壓到float16 / or further qunatization，因此才給個限制。\n","                nn.ReLU6(),\n","                # Pointwise Convolution\n","                nn.Conv2d(bandwidth[0], bandwidth[1], 1),\n","                # 過完Pointwise Convolution不需要再做ReLU，經驗上Pointwise + ReLU效果都會變差。\n","                nn.MaxPool2d(2, 2, 0),\n","                # 每過完一個Block就Down Sampling\n","            ),\n","\n","            nn.Sequential(\n","                nn.Conv2d(bandwidth[1], bandwidth[1], 3, 1, 1, groups=bandwidth[1]),\n","                nn.BatchNorm2d(bandwidth[1]),\n","                nn.ReLU6(),\n","                nn.Conv2d(bandwidth[1], bandwidth[2], 1),\n","                nn.MaxPool2d(2, 2, 0),\n","            ),\n","\n","            nn.Sequential(\n","                nn.Conv2d(bandwidth[2], bandwidth[2], 3, 1, 1, groups=bandwidth[2]),\n","                nn.BatchNorm2d(bandwidth[2]),\n","                nn.ReLU6(),\n","                nn.Conv2d(bandwidth[2], bandwidth[3], 1),\n","                nn.MaxPool2d(2, 2, 0),\n","            ),\n","\n","            # 到這邊為止因為圖片已經被Down Sample很多次了，所以就不做MaxPool\n","            nn.Sequential(\n","                nn.Conv2d(bandwidth[3], bandwidth[3], 3, 1, 1, groups=bandwidth[3]),\n","                nn.BatchNorm2d(bandwidth[3]),\n","                nn.ReLU6(),\n","                nn.Conv2d(bandwidth[3], bandwidth[4], 1),\n","            ),\n","\n","            nn.Sequential(\n","                nn.Conv2d(bandwidth[4], bandwidth[4], 3, 1, 1, groups=bandwidth[4]),\n","                nn.BatchNorm2d(bandwidth[4]),\n","                nn.ReLU6(),\n","                nn.Conv2d(bandwidth[4], bandwidth[5], 1),\n","            ),\n","\n","            nn.Sequential(\n","                nn.Conv2d(bandwidth[5], bandwidth[5], 3, 1, 1, groups=bandwidth[5]),\n","                nn.BatchNorm2d(bandwidth[5]),\n","                nn.ReLU6(),\n","                nn.Conv2d(bandwidth[5], bandwidth[6], 1),\n","            ),\n","\n","            nn.Sequential(\n","                nn.Conv2d(bandwidth[6], bandwidth[6], 3, 1, 1, groups=bandwidth[6]),\n","                nn.BatchNorm2d(bandwidth[6]),\n","                nn.ReLU6(),\n","                nn.Conv2d(bandwidth[6], bandwidth[7], 1),\n","            ),\n","\n","            # 這邊我們採用Global Average Pooling。\n","            # 如果輸入圖片大小不一樣的話，就會因為Global Average Pooling壓成一樣的形狀，這樣子接下來做FC就不會對不起來。\n","            nn.AdaptiveAvgPool2d((1, 1)),\n","        )\n","        self.fc = nn.Sequential(\n","            # 這邊我們直接Project到11維輸出答案。\n","            nn.Linear(bandwidth[7], 11),\n","        )\n","\n","    def forward(self, x):\n","        out = self.cnn(x)\n","        out = out.view(out.size()[0], -1)\n","        return self.fc(out)"]},{"cell_type":"code","metadata":{"id":"ZWdQtDtgoGCp","colab_type":"code","outputId":"dc49692a-c7e4-4838-a09b-d64ac9aae1aa","executionInfo":{"status":"ok","timestamp":1583670596828,"user_tz":-480,"elapsed":25,"user":{"displayName":"李宏毅","photoUrl":"","userId":"14645962671355321712"}},"colab":{"base_uri":"https://localhost:8080/","height":84}},"source":["teacher_net = models.resnet18(pretrained=False, num_classes=11).cuda()\n","student_net = StudentNet(base=16).cuda()\n","\n","teacher_net.load_state_dict(torch.load(f'./teacher_resnet18.bin'))\n","optimizer = optim.AdamW(student_net.parameters(), lr=1e-3)"],"execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Wvc1W5yO2QaE","colab_type":"text"},"source":["# Start Training\n","\n","* 剩下的步驟與你在做Hw3 - CNN的時候一樣。\n","\n","## 小提醒\n","\n","* torch.no_grad是指接下來的運算或該tensor不需要算gradient。\n","* model.eval()與model.train()差在於Batchnorm要不要紀錄，以及要不要做Dropout。\n","\n"]},{"cell_type":"code","metadata":{"id":"-TzmWtT62Qmy","colab_type":"code","outputId":"343e1c73-2e9b-43a4-82f0-004ce72b4842","executionInfo":{"status":"error","timestamp":1583671594964,"user_tz":-480,"elapsed":639,"user":{"displayName":"李宏毅","photoUrl":"","userId":"14645962671355321712"}},"colab":{"base_uri":"https://localhost:8080/","height":1000},"tags":[]},"source":["def run_epoch(dataloader, update=True, alpha=0.5):\n","    total_num, total_hit, total_loss = 0, 0, 0\n","    for now_step, batch_data in enumerate(dataloader):\n","        # 清空 optimizer\n","        optimizer.zero_grad()\n","        # 處理 input\n","        inputs, hard_labels = batch_data\n","        inputs = inputs.cuda()\n","        hard_labels = torch.LongTensor(hard_labels).cuda()\n","        # 因為Teacher沒有要backprop，所以我們使用torch.no_grad\n","        # 告訴torch不要暫存中間值(去做backprop)以浪費記憶體空間。\n","        with torch.no_grad():\n","            soft_labels = teacher_net(inputs)\n","\n","        if update:\n","            logits = student_net(inputs)\n","            # 使用我們之前所寫的融合soft label&hard label的loss。\n","            # T=20是原始論文的參數設定。\n","            loss = loss_fn_kd(logits, hard_labels, soft_labels, 20, alpha)\n","            loss.backward()\n","            optimizer.step()    \n","        else:\n","            # 只是算validation acc的話，就開no_grad節省空間。\n","            with torch.no_grad():\n","                logits = student_net(inputs)\n","                loss = loss_fn_kd(logits, hard_labels, soft_labels, 20, alpha)\n","            \n","        total_hit += torch.sum(torch.argmax(logits, dim=1) == hard_labels).item()\n","        total_num += len(inputs)\n","\n","        total_loss += loss.item() * len(inputs)\n","    return total_loss / total_num, total_hit / total_num\n","\n","\n","# TeacherNet永遠都是Eval mode.\n","teacher_net.eval()\n","now_best_acc = 0\n","for epoch in range(5):\n","    student_net.train()\n","    train_loss, train_acc = run_epoch(train_dataloader, update=True)\n","    student_net.eval()\n","    valid_loss, valid_acc = run_epoch(valid_dataloader, update=False)\n","\n","    # 存下最好的model。\n","    if valid_acc > now_best_acc:\n","        now_best_acc = valid_acc\n","        torch.save(student_net.state_dict(), 'student_model.bin')\n","    print('epoch {:>3d}: train loss: {:6.4f}, acc {:6.4f} valid loss: {:6.4f}, acc {:6.4f}'.format(\n","        epoch, train_loss, train_acc, valid_loss, valid_acc))\n"],"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":"epoch   0: train loss: 12.8878, acc 0.4302 valid loss: 14.0426, acc 0.4557\nepoch   1: train loss: 11.8721, acc 0.4750 valid loss: 13.2372, acc 0.4933\nepoch   2: train loss: 11.1797, acc 0.5074 valid loss: 12.0157, acc 0.5105\nepoch   3: train loss: 10.8692, acc 0.5254 valid loss: 11.4599, acc 0.5388\nepoch   4: train loss: 10.3543, acc 0.5434 valid loss: 11.1040, acc 0.5496\n"}]},{"cell_type":"markdown","metadata":{"id":"0GObCiGNtPkZ","colab_type":"text"},"source":["# Inference\n","\n","同Hw3，請參考該作業:)。\n"]}]}